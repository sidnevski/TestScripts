{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad32156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import paho.mqtt.client as client\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prereq_results():\n",
    "#     print(\"prereq\")\n",
    "    body = {\"algorithm\": \"function_approximation\"}\n",
    "    url = 'http://0.0.0.0:5000/pre_req'\n",
    "    r = requests.post(url, json=body)\n",
    "    prereq_res = r.json()\n",
    "#     print(prereq_res.keys())\n",
    "    return prereq_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e9584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase0_results():\n",
    "    body = {\"algorithm\": \"function_approximation\",\"chunk_count\": 30}\n",
    "    url = 'http://0.0.0.0:5000/phase0'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase0_res = r.json()\n",
    "#     print(phase0_res.keys())\n",
    "    return phase0_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4577e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase1_results():\n",
    "    body = {\n",
    "        \"algorithm\": \"function_approximation\",\n",
    "        \"partition_size\": 100,\n",
    "        \"chunk_size\": 100,\n",
    "        \"net_sizes\": [5],\n",
    "        \"lr\": [0.1],\n",
    "        \"no_data_passes\": 20,\n",
    "        \"SIZE_OF_PARTITIONS\": 10,\n",
    "        \"no_chunk_passes\": 20,\n",
    "        \"neigh_rate\": 0.8\n",
    "    }\n",
    "    url = 'http://0.0.0.0:5000/phase1'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase1_res = r.json()\n",
    "#     print(phase1_res)\n",
    "    return phase1_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de34ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase2_results(\n",
    "    dataset,\n",
    "    train_dataset_path,\n",
    "    validation_dataset_path,\n",
    "    test_dataset_path,\n",
    "    partition_size=100,\n",
    "                       chunk_size=100,\n",
    "                       net_sizes=[3,4,5],\n",
    "                       lr=[0.1, 0.05, 0.075],\n",
    "                       num_passes=100,\n",
    "                       top_ranks=100,\n",
    "                       select_fs_cnt=100,\n",
    "                       train_split=0.9,\n",
    "                       convergence_threshold=0.05,\n",
    "                       stack_passes=100\n",
    "                      ):\n",
    "    body = {\n",
    "        \"algorithm\": \"function_approximation\",\n",
    "        \"partition_size\": partition_size,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"net_sizes\": net_sizes,\n",
    "        \"lr\": lr,\n",
    "        \"no_passes\": num_passes,\n",
    "        \"top_ranks\": top_ranks,\n",
    "        \"select_fs_cnt\": select_fs_cnt,\n",
    "        \"train_split\": train_split,\n",
    "        \"boost_trials\": 1,\n",
    "        \"convergence_threshold\": convergence_threshold,\n",
    "        \"lambda\": [1],\n",
    "        \"stack_passes\": stack_passes,\n",
    "        \"weight_init_std\": 0.2,\n",
    "        'train_dataset_path': train_dataset_path,\n",
    "        'validation_dataset_path': validation_dataset_path,\n",
    "        'test_dataset_path': test_dataset_path,\n",
    "        'dataset': dataset\n",
    "    }\n",
    "    \n",
    "    url = 'http://0.0.0.0:5000/phase2'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase2_res = r.json()\n",
    "#     print(phase2_res.keys())\n",
    "    return phase2_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553449b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Page.total.likes', 'Category', 'Post.Month', 'Post.Weekday', 'Post.Hour', 'Paid', 'Lifetime.Post.Total.Reach', 'Lifetime.Post.Total.Impressions', 'Lifetime.Engaged.Users', 'Lifetime.Post.Consumers', 'Lifetime.Post.Consumptions', 'Lifetime.Post.Impressions.by.people.who.have.liked.your.Page', 'Lifetime.Post.reach.by.people.who.like.your.Page', 'Lifetime.People.who.have.liked.your.Page.and.engaged.with.your.post', 'comment', 'like', 'share', 'Type_Link', 'Type_Photo', 'Type_Status', 'Type_Video', 'Total.Interactions']\n",
      "ECG connected\n",
      "connecting to broker\n",
      "Subscribing to all topics\n",
      "adding topic to latest_elements \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 665, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 285, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "http.client.RemoteDisconnected: Remote end closed connection without response\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 719, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/util/retry.py\", line 400, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/asim/.local/lib/python3.8/site-packages/six.py\", line 702, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 665, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 416, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.8/http/client.py\", line 285, in _read_status\n",
      "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_14339/860823880.py\", line 18, in process_results\n",
      "    phase2_res = get_phase2_results(\n",
      "  File \"/tmp/ipykernel_14339/1852534013.py\", line 39, in get_phase2_results\n",
      "    r = requests.post(url, json=body)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/api.py\", line 116, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  0%|                                                                                                                                                                                                             | 0/1 [11:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['dataset', 'seed', 'hostTime', 'KernelTime', 'best_chunk', 'test_rmse',\\n       'test_rmses', 'score', 'scores', 'train_rmse', 'train_rmses',\\n       'val_rmse', 'val_rmses', 'best_fs_features', 'best_val_lr',\\n       'best_feature_list', 'important_features',\\n       'best_model_active_gaussian_weights',\\n       'best_model_active_linear_weights', 'bound_hitting_gaussian_weights',\\n       'phase1_chunks_for_convergence', 'phase2_chunks_for_convergence',\\n       'phase3_chunks_for_convergence', 'min_test_rmse', 'min_train_rmse',\\n       'min_validation_rmse', 'min_test_rmse_model_score', 'min_test_rmse_lr',\\n       'min_test_rmse_fs', 'min_test_rmse_chunk'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m filepath \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_NL45_conv4_FeaturesMax_RandomInit_Epochs20.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset)\n\u001b[1;32m    146\u001b[0m filepath\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 147\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_column_order\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(filepath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3030\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3029\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3030\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1266\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1264\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1308\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m   1307\u001b[0m     axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1310\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# We (temporarily) allow for some missing keys with .loc, except in\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# some cases (e.g. setting) in which \"raise_missing\" will be False\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['dataset', 'seed', 'hostTime', 'KernelTime', 'best_chunk', 'test_rmse',\\n       'test_rmses', 'score', 'scores', 'train_rmse', 'train_rmses',\\n       'val_rmse', 'val_rmses', 'best_fs_features', 'best_val_lr',\\n       'best_feature_list', 'important_features',\\n       'best_model_active_gaussian_weights',\\n       'best_model_active_linear_weights', 'bound_hitting_gaussian_weights',\\n       'phase1_chunks_for_convergence', 'phase2_chunks_for_convergence',\\n       'phase3_chunks_for_convergence', 'min_test_rmse', 'min_train_rmse',\\n       'min_validation_rmse', 'min_test_rmse_model_score', 'min_test_rmse_lr',\\n       'min_test_rmse_fs', 'min_test_rmse_chunk'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "def process_results(data_dir, dataset, seed, df):\n",
    "    proc, gateway_proc, details = None, None, {}\n",
    "    try:\n",
    "        gateway_proc = subprocess.Popen(['python', os.path.join(data_directory, 'gateway_simulation.py')])\n",
    "        # backend_proc = subprocess.Popen(['python', '/home/asim/ssriva59/MainAppCuda/main.py'])\n",
    "        proc = subprocess.Popen(['python', os.path.join(data_directory, 'ECG_Stream_V2.py'), dataset, str(seed)])\n",
    "        time.sleep(10)\n",
    "        \n",
    "        prereq_res = get_prereq_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase0_res = get_phase0_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase1_res = get_phase1_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase2_res = get_phase2_results(\n",
    "            dataset,\n",
    "            train_dataset_path=os.path.join(data_dir, dataset, \"Train\", dataset + '_seed_' + str(seed) + '_train.csv'),\n",
    "            validation_dataset_path=os.path.join(data_dir, dataset, \"Train\", dataset + '_seed_' + str(seed) + '_validation.csv'),\n",
    "            test_dataset_path=os.path.join(data_dir, dataset, \"Test\", dataset + '_Test_seed' + str(seed) + '_modified.csv'),\n",
    "            net_sizes=[3], #, 4, 5, 6, 7],\n",
    "            lr=[0.1] # , 0.05, 0.075, 0.5]\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        \n",
    "        features = min(len(phase1_res['rankings']), 15)\n",
    "        details = {\n",
    "            'dataset': dataset,\n",
    "            'seed': seed,\n",
    "            'train_rmse': phase2_res['trainError'],\n",
    "            'test_rmse': phase2_res['testError'],\n",
    "            'KernelTime': phase1_res['kernel_time'] + phase2_res['kernel_time'],\n",
    "            'hostTime': phase0_res['host_time'] + phase1_res['host_time'] + phase2_res['host_time']\n",
    "        }\n",
    "        \n",
    "        details['val_rmse'] = phase2_res['global_min_val_rmse']\n",
    "        details['phase1_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase1']\n",
    "        details['phase2_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase2']\n",
    "        details['phase3_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase3']\n",
    "        details['best_feature_list'] = phase2_res['best_feature_list']\n",
    "        details['best_val_lr'] = phase2_res['best_val_lr']\n",
    "        details['best_fs_features'] = phase2_res['best_fs_num_features']\n",
    "        details['val_rmses'] = phase2_res['val_rmses']\n",
    "        details['train_rmses'] = phase2_res['train_rmses']\n",
    "        details['test_rmses'] = phase2_res['test_rmses']\n",
    "        details['best_chunk'] = phase2_res['best_chunk']\n",
    "        details['best_model_active_gaussian_weights'] = phase2_res['best_model_active_gaussian_weights']\n",
    "        details['best_model_active_linear_weights'] = phase2_res['best_model_active_linear_weights']\n",
    "        details['bound_hitting_gaussian_weights'] = phase2_res['bound_hitting_gaussian_weights']\n",
    "        details['score'] = phase2_res['score']\n",
    "        details['scores'] =  phase2_res['scores']\n",
    "        details['min_test_rmse'] = phase2_res['min_test_rmse']\n",
    "        details['min_train_rmse'] = phase2_res['min_train_rmse']\n",
    "        details['min_validation_rmse'] = phase2_res['min_validation_rmse']\n",
    "        details['min_test_rmse_model_score'] = phase2_res['min_test_rmse_model_score']\n",
    "        details['min_test_rmse_lr'] = phase2_res['min_test_rmse_lr']\n",
    "        details['min_test_rmse_fs'] = phase2_res['min_test_rmse_fs']\n",
    "        details['min_test_rmse_chunk'] = phase2_res['min_test_rmse_chunk']\n",
    "        \n",
    "        imp_features = []\n",
    "        for rank in phase1_res['rankings']:\n",
    "            imp_features.append(rank['Feature'])\n",
    "        details['important_features'] = imp_features\n",
    "        \n",
    "        # backend_proc.kill()\n",
    "        proc.kill()\n",
    "        gateway_proc.kill()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        # if backend_proc:\n",
    "        #     backend_proc.kill()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "    return details\n",
    "\n",
    "train_data, test_data, list_of_feature_names = [], [], []\n",
    "data_directory = \"/home/asim/ssriva59/setup-stuff/gateway_and_dataset\"\n",
    "datasets = [\n",
    "    \"Facebook_data\", # -> done\n",
    "    # \"Features_TestSet\", # -> done\n",
    "    # \"House_Price_Adv_Regression\", # -> done\n",
    "    # \"Instant_Liking\",\n",
    "    # \"Insurance\", # -> done\n",
    "#     # \"Isolet\" need to redo cuz of some error in RF,\n",
    "    # \"new_data_trans\", # -> done\n",
    "    # \"OnlineNewsPopularity\", # -> done\n",
    "    # \"ParkinsonData\", # -> done\n",
    "    # \"Sberbank_Russian_Housing_Market\", # -> done\n",
    "    # \"slice_localization_data\", # -> done\n",
    "    # \"Telecom_data\", # -> done\n",
    "    # \"yearMSD_new\", # -> done\n",
    "    # \"arrhythmia\", # -> done\n",
    "#     # \"Big_mart_sales\",\n",
    "    # \"blogData\", # -> done\n",
    "    # \"communities\", # -> done\n",
    "    # \"dengue_features\", # -> done\n",
    "    # \"ECG0_p02\",\n",
    "    # \"ENERGY_DATA_COMPLETE\" # -> done\n",
    "]\n",
    "\n",
    "df_column_order = [\n",
    "    'dataset',\n",
    "    'seed',\n",
    "    'hostTime',\n",
    "    'KernelTime',\n",
    "    'best_chunk',\n",
    "    'test_rmse',\n",
    "    'test_rmses',\n",
    "    'score',\n",
    "    'scores',\n",
    "    'train_rmse',\n",
    "    'train_rmses',\n",
    "    'val_rmse',\n",
    "    'val_rmses',\n",
    "    'best_fs_features',\n",
    "    'best_val_lr',\n",
    "    'best_feature_list',\n",
    "    'important_features',\n",
    "    'best_model_active_gaussian_weights',\n",
    "    'best_model_active_linear_weights',\n",
    "    'bound_hitting_gaussian_weights',\n",
    "    'phase1_chunks_for_convergence',\n",
    "    'phase2_chunks_for_convergence',\n",
    "    'phase3_chunks_for_convergence',\n",
    "    'min_test_rmse',\n",
    "    'min_train_rmse',\n",
    "    'min_validation_rmse',\n",
    "    'min_test_rmse_model_score',\n",
    "    'min_test_rmse_lr',\n",
    "    'min_test_rmse_fs',\n",
    "    'min_test_rmse_chunk'\n",
    "]\n",
    "\n",
    "for index, dataset in enumerate(datasets):\n",
    "    print(dataset)\n",
    "    seeds = [50] #, 50, 100, 150, 200, 250, 300, 350, 400, 450]\n",
    "    df = pd.DataFrame()\n",
    "    for s in tqdm(seeds):\n",
    "        details = process_results(data_directory, dataset, s, df)\n",
    "        df = df.append(details, ignore_index=True)\n",
    "        filepath = Path('%s_NL45_conv4_FeaturesMax_RandomInit_Epochs20.csv' % dataset)\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df[df_column_order].to_csv(filepath, index=False)\n",
    "        \n",
    "# Only for ECG\n",
    "\n",
    "# dataset = 'ECG'\n",
    "# print(dataset)\n",
    "# seeds = [1]\n",
    "# df = pd.DataFrame()\n",
    "# for s in tqdm(seeds):\n",
    "#     details = process_results(data_directory, dataset, s, df)\n",
    "#     df = df.append(details, ignore_index=True)\n",
    "#     filepath = Path('%s-4-5.csv' % dataset)\n",
    "#     filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_missing_results(data_dir, dataset, seed, df):\n",
    "    proc, gateway_proc, details = None, None, {}\n",
    "    try:\n",
    "        gateway_proc = subprocess.Popen(['python', os.path.join(data_directory, 'gateway_simulation.py')])\n",
    "        backend_proc = subprocess.Popen(['python', '/home/asim/ssriva59/MainAppCuda/main.py'])\n",
    "        proc = subprocess.Popen(['python', os.path.join(data_directory, 'ECG_Stream_V2.py'), dataset, str(seed)])\n",
    "        time.sleep(10)\n",
    "        \n",
    "        prereq_res = get_prereq_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase0_res = get_phase0_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase1_res = get_phase1_results()\n",
    "        time.sleep(10)\n",
    "        \n",
    "        phase2_res = get_phase2_results(\n",
    "            train_dataset_path=os.path.join(data_dir, dataset, \"Train\", dataset + '_seed_' + str(seed) + '_train.csv'),\n",
    "            validation_dataset_path=os.path.join(data_dir, dataset, \"Train\", dataset + '_seed_' + str(seed) + '_validation.csv'),\n",
    "            test_dataset_path=os.path.join(data_dir, dataset, \"Test\", dataset + '_Test_seed' + str(seed) + '_modified.csv'),\n",
    "            net_sizes=[3,4,5,6]\n",
    "        )\n",
    "        time.sleep(10)\n",
    "        \n",
    "        features = min(len(phase1_res), 15)\n",
    "        details = {\n",
    "            'dataset': dataset,\n",
    "            'seed': seed,\n",
    "            'train_rmse': phase2_res['trainError'],\n",
    "            'test_rmse': phase2_res['testError'],\n",
    "            'KernelTime': phase2_res['kernelTime'],\n",
    "            'hostTime': phase2_res['hostTime']\n",
    "        }\n",
    "        \n",
    "        details['global_min_val_rmse'] = phase2_res['global_min_val_rmse']\n",
    "        details['phase1_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase1']\n",
    "        details['phase2_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase2']\n",
    "        details['phase3_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase3']\n",
    "        details['best_feature_list'] = phase2_res['best_feature_list']\n",
    "        details['best_val_lr'] = phase2_res['best_val_lr']\n",
    "        details['best_val_fs'] = phase2_res['best_val_fs']\n",
    "        details['val_rmses'] = phase2_res['val_rmses']\n",
    "        details['train_rmses'] = phase2_res['train_rmses']\n",
    "        details['test_rmses'] = phase2_res['test_rmses']\n",
    "        details['best_chunk'] = phase2_res['best_chunk']\n",
    "        \n",
    "        imp_features = []\n",
    "        for i in range(features):\n",
    "            imp_features.append(phase1_res[i]['Feature'])\n",
    "        details['important_features'] = imp_features\n",
    "        proc.kill()\n",
    "        backend_proc.kill()\n",
    "        gateway_proc.kill()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        if backend_proc:\n",
    "            backend_proc.kill()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "    return details\n",
    "\n",
    "\n",
    "train_data, test_data, list_of_feature_names = [], [], []\n",
    "data_directory = \"/home/asim/ssriva59/setup-stuff/gateway_and_dataset\"\n",
    "datasets = [\n",
    "    # \"Facebook_data\",\n",
    "    # \"Features_TestSet\",\n",
    "    # \"House_Price_Adv_Regression\",\n",
    "    # \"Instant_Liking\",\n",
    "    # \"Insurance\",\n",
    "    # \"Isolet\",\n",
    "    # \"new_data_trans\",\n",
    "    # \"OnlineNewsPopularity\",\n",
    "    # \"ParkinsonData\",\n",
    "    # \"Sberbank_Russian_Housing_Market\",\n",
    "    \"slice_localization_data\",\n",
    "    # \"Telecom_data\",\n",
    "    # \"yearMSD_new\",\n",
    "    # \"arrhythmia\",\n",
    "    # \"Big_mart_sales\",\n",
    "    \"blogData\",\n",
    "    # \"communities\",\n",
    "    # \"dengue_features\",\n",
    "    # \"ECG0_p02\",\n",
    "    # \"ENERGY_DATA_COMPLETE\"\n",
    "]\n",
    "\n",
    "dataset_seed = {\n",
    "    \"Facebook_data\": [150],\n",
    "    \"Features_TestSet\": [400],\n",
    "    \"House_Price_Adv_Regression\": [100],\n",
    "    \"Insurance\": [100],\n",
    "    \"OnlineNewsPopularity\": [300],\n",
    "    \"new_data_trans\": [50, 250],\n",
    "    \"ParkinsonData\": [400],\n",
    "    \"arrhythmia\": [50, 100, 150],\n",
    "    \"communities\": [450],\n",
    "    \"dengue_features\": [50, 300],\n",
    "    \"new_data_trans\": [450],\n",
    "    \"Insurance\": [200],\n",
    "    \"slice_localization_data\": [300, 350, 400, 450],\n",
    "    \"Telecom_data\": [250, 400],\n",
    "    \"blogData\": [200, 250, 300, 350, 400, 450]\n",
    "}\n",
    "\n",
    "# missing seed here\n",
    "# seeds = [250]\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    df = pd.DataFrame()\n",
    "    for s in tqdm(dataset_seed[dataset]):\n",
    "        details = process_missing_results(data_directory, dataset, s, df)\n",
    "        df = df.append(details, ignore_index=True)\n",
    "        filepath = Path('%s_missing-4-5.csv' % dataset)\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
